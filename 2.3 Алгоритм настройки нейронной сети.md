## Градиентный спуск
![Pasted image 20250629135246.png](Pasted%20image%2020250629135246.png)Это значит, что в w0​ (это вектор весов), будут написаны все коэффициенты, которые мы используем в нашей нейронной сети. То есть, это какие-то веса для одного нейрона, какие-то веса для другого нейрона, и так далее, какие то смещения для одного нейрона, какие-то смещения для другого нейрона, и так далее. То есть это такой вот большой вектор. 
Мы берём нашу функцию потерь и считаем её производную. Точнее, градиент. Градиент -- это вектор, который состоит из производных по каждой из координат функции. Мы посчитали градиент функции потерь в точке w0​, в которой мы сейчас находимся. 
Градиент функции потерь показывает в сторону наибольшего роста функции потерь. Нам же нужно уменьшать функцию. Соответственно, нам нужно сделать шаг из w0​ в направлении, обратном к градиенту. Мы получаем новый вектор весов w1, который равен w0​ минус α умножить на градиент функции в точке w0​. Можно увидеть, что наша функция должна была уменьшиться.

**СВОЙСТВО** Градиентный спуск будет находить минимум, но минимум будет не всегда оптимальным 

## Ограничение функции потерь
1. Функция потерь дифференцируема
2. функции потерь, в каком-то достаточно большом множестве точек, должна быть равна не нулю, а какому-то значению

![Pasted image 20250629140832.png](Pasted%20image%2020250629140832.png)
![Pasted image 20250629141042.png](Pasted%20image%2020250629141042.png)